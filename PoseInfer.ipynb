{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import glob\n",
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nutil import normImage\n",
    "import torch.utils.data as data_utils\n",
    "from dep.unet import ResNetUNetNew as ResNetUNet\n",
    "from nutil import show_full_render1\n",
    "from augment import generateImages\n",
    "from nutil import get_emb_vis, huber, sample_images_at_mc_locs, returnCrossEntropy,returnCrossEntropyWithNeg\n",
    "from nerf import NeuralRadianceFieldFeat\n",
    "from pren import ImplicitRendererStratified, EmissionAbsorptionRaymarcherStratified\n",
    "from PIL import Image\n",
    "from IPython import display\n",
    "from tqdm.notebook import tqdm\n",
    "from cowrendersynth import generate_cow_rendersWithRT, generate_bop_realsamples\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from nutil import mip360loss\n",
    "from pytorch3d.structures import Volumes\n",
    "from pytorch3d.transforms import so3_exp_map\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from dataGen import AugmentedSamples\n",
    "import pytorch3d,cv2\n",
    "from nutil import rotfromeulernp\n",
    "import argparse\n",
    "from pytorch3d.renderer import (\n",
    "    FoVPerspectiveCameras,\n",
    "    PerspectiveCameras,\n",
    "    NDCMultinomialRaysampler,\n",
    "    MonteCarloRaysampler,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    ImplicitRenderer,\n",
    "    RayBundle,\n",
    "    ray_bundle_to_ray_points,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "objid=\"1\"\n",
    "expID = \"ruapc/\"+objid+\"poseEst\"\n",
    "batch_size = 16\n",
    "sampleSize = 1024\n",
    "key_noise = 1e-3\n",
    "Siren=True\n",
    "datasetPath = \"bop/ruapc/\"\n",
    "imD = 224\n",
    "in_ndc = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/damvan/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1c25d847354408be916e1e29f568fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "# Instantiate the radiance field model.\n",
    "neural_radiance_field = NeuralRadianceFieldFeat(siren=Siren)\n",
    "encoder_rgb = ResNetUNet(n_class=(13),n_decoders=1,)  ##?n_class =13 - dm cos here is the query image with embedding size =13 ddo\n",
    "#encoder_rgb2 = ResNetUNet(n_class=(3),n_decoders=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nerf model as key model\n",
    "data_nerf = torch.load(expID+\"/nerflatest.pth\")\t\n",
    "neural_radiance_field.load_state_dict(data_nerf[\"model_state_dict\"])\n",
    "# load CNN as query model\t\n",
    "data_rgb = torch.load(expID+\"/encoderRGBlatest.pth\")\t\n",
    "encoder_rgb.load_state_dict(data_rgb[\"model_state_dict\"])\t\n",
    "neural_radiance_field=neural_radiance_field\n",
    "encoder_rgb=encoder_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue\n"
     ]
    }
   ],
   "source": [
    "#set models to eval\n",
    "neural_radiance_field.eval()\n",
    "encoder_rgb.eval()\n",
    "print(\"continue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from second sequence\n",
    "fsamps = int(2561 * 0.5)\n",
    "fewIds = np.arange(int(2561 * 0.5)) +1280\n",
    "target_images, target_silhouettes, RObj, TObj, KObj, fewIds = generate_bop_realsamples(datasetPath,objid=objid, crop=True,maskStr=\"mask\",offset=5, synth=False,makeNDC=in_ndc,dataset=\"tless\", maxB=imD, background=False, fewSamps=True, fewCT=fsamps, fewids=fewIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 79972, 3])\n",
      "torch.Size([1, 16384, 3])\n"
     ]
    }
   ],
   "source": [
    "## load fullNegVec\n",
    "if os.path.exists(expID+\"/negVec.npy\"):\n",
    "    fullNegVec= torch.from_numpy(np.load(expID+\"/negVec.npy\").astype(\"float32\")).unsqueeze(0)\n",
    "negSamps = torch.randperm(fullNegVec.shape[1], dtype=torch.int64)[0:batch_size*sampleSize]\n",
    "negVec=fullNegVec[:,negSamps].clone()\n",
    "# negVec+= torch.randn_like(negVec) * key_noise # negvec =negvec +negvec*keynoise\n",
    "print(fullNegVec.shape)\n",
    "print(negVec.shape) #only take 16384 points from the all the 3D points- to reduce computation at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input =torch.movedim(target_images, 3, 1)\n",
    "input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    queries = [encoder_rgb.to(device)(input[i].unsqueeze(0).to(device)) for i in range(input.shape[0])]\n",
    "    feats = neural_radiance_field.batched_customForward(negVec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del encoder_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1280, 13, 224, 224])\n",
      "torch.Size([1, 16384, 13])\n"
     ]
    }
   ],
   "source": [
    "queries = torch.stack([tensor.to('cpu') for tensor in queries]).squeeze()\n",
    "print(queries.shape)\n",
    "print(feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50176, 13])\n",
      "torch.Size([16384, 13])\n"
     ]
    }
   ],
   "source": [
    "#first image of second sequence as 1280th image \n",
    "ith_queries = queries[0].reshape(-1,13)\n",
    "resized_feats = feats.squeeze() \n",
    "print(ith_queries.shape)\n",
    "print(resized_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCors(queries, feats, leaves=1):\n",
    "    similarity_mat=(queries @ feats.T)\n",
    "    cMat = torch.log_softmax(similarity_mat, dim=-1)\n",
    "    ids=torch.argmax(cMat, dim=-1)\n",
    "    #vals, idx = torch.topk(cMat, k=leaves, dim=-1)\n",
    "    # if leaves == 1:\n",
    "    #     return idx[..., 0].cpu(), vals\n",
    "    # else:\n",
    "    #     return idx.cpu(), vals\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#obtain id of 3D corresponding in the negVec\n",
    "ids= getCors(ith_queries, resized_feats)\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask =target_silhouettes[0]\n",
    "#fisrt convert mask to 0,1\n",
    "mask[mask < 0.5] = 0\n",
    "mask[mask >= 0.5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15554"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_pixels = [torch.tensor([i, j]) for i in range(224) for j in range(224) if mask[i, j] == 1]  ## pixels inside object\n",
    "# back_ground = [[i, j] for i in range(224) for j in range(224) if mask[i, j] == 0] ## background\n",
    "len(obj_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15554\n"
     ]
    }
   ],
   "source": [
    "# get the 3D corres for the obj_pixels\n",
    "corIds=[ids[pixel[0]*224+pixel[1]] for pixel in obj_pixels]\n",
    "print(len(corIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##finally we have 2D and its 3D Corre\n",
    "pt2D = torch.stack(obj_pixels, dim = 0).numpy().astype(\"float32\")\n",
    "pt3D = negVec[:,corIds,:].squeeze().numpy().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale back 3D points\n",
    "diameter = 103.058\n",
    "pt3D = diameter/0.5*pt3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11. 99.]\n",
      "[ -21.538494   44.856907 -178.02055 ]\n"
     ]
    }
   ],
   "source": [
    "## print an example result\n",
    "print(pt2D[0])\n",
    "print(pt3D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rot180 = rotfromeulernp(np.array([0, 0, np.pi]))\n",
    "# for a in range(target_images.shape[0]):\n",
    "#   RObj[a] = (RObj[a].T).dot(rot180)\n",
    "#   TObj[a, 0:2] = -TObj[a, 0:2]\n",
    "# meshdetails = json.load(open(datasetPath + \"/models\" + \"/models_info.json\"))\n",
    "# diam = meshdetails[objid]['diameter']\n",
    "# diamScaling=1.8\n",
    "# offset=0\n",
    "# scale=diam/diamScaling\n",
    "# TObj = TObj/scale\n",
    "\n",
    "\n",
    "# stratified=False\n",
    "# enable3D=False\n",
    "# enableSSP=False\n",
    "# negativeSampling=True\n",
    "# maskRays=True\n",
    "# rayFreeze=False\n",
    "\n",
    "# target_cameras = PerspectiveCameras(device=device, R=torch.from_numpy(RObj.astype(\"float32\")),\n",
    "#                                     T=torch.from_numpy(TObj.astype(\"float32\")),\n",
    "#                                     K=torch.from_numpy(KObj.astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pose6DEstimator(image_points,world_points):\n",
    "    # Distortion coefficients (if any)\n",
    "    dist_coeffs = np.array([0, 0, 0, 0, 0], dtype=np.float32)\n",
    "    K = np.array([531.15, 0.0, 320.0, 0.0, 531.15, 240.0, 0.0, 0.0, 1.0]).reshape(3,3).astype('float32')\n",
    "    ransac_reproj_threshold = 5.0  # Threshold for RANSAC reprojection error\n",
    "    ransac_iterations = 1000  # Number of RANSAC iterations\n",
    "    # Estimate pose using solvePnPRansac\n",
    "    success, rvec, tvec, inliers = cv2.solvePnPRansac(world_points, image_points, K, dist_coeffs,\n",
    "                                                 reprojectionError=ransac_reproj_threshold,\n",
    "                                                 iterationsCount=ransac_iterations)\n",
    "    return success, rvec, tvec,inliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "success, rvec, tvec, inliers = Pose6DEstimator(pt2D,pt3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.77541512,  0.63142768, -0.00551972],\n",
       "       [ 0.63132491,  0.77540491,  0.01326926],\n",
       "       [ 0.0126586 ,  0.00680445, -0.99989672]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R, _ = cv2.Rodrigues(rvec)\n",
    "t = tvec\n",
    "R"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
